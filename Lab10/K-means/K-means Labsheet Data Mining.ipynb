{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means Clustering\n",
    "\n",
    "K-Means Clustering is an unsupervised machine learning algorithm. In contrast to traditional supervised machine learning algorithms, K-Means attempts to classify data without having first been trained with labeled data. Once the algorithm has been run and the groups are defined, any new data can be easily assigned to the most relevant group.\n",
    "\n",
    "The real world applications of K-Means include:\n",
    "- customer profiling\n",
    "- market segmentation\n",
    "- computer vision\n",
    "- search engines\n",
    "- astronomy\n",
    "\n",
    "\n",
    "### How it works:\n",
    "\n",
    "It is an iterative algorithm.\n",
    "It chooses k centers at random and keeps updating the centres while minimising the error (error is generally taken as MSE). \n",
    "Here is an illustration \n",
    "\n",
    "![image](http://shabal.in/visuals/kmeans/left.gif)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GIF Credits: http://shabal.in/visuals/kmeans/left.gif\n",
    "\n",
    "### Choosing the right number of clusters\n",
    "\n",
    "Often times the data you’ll be working with will have multiple dimensions making it difficult to visual. As a consequence, the optimum number of clusters is no longer obvious. Fortunately, we have a way of determining this mathematically.\n",
    "\n",
    "We graph the relationship between the number of clusters and Within Cluster Sum of Squares (WCSS) then we select the number of clusters where the change in WCSS begins to level off (elbow method).\n",
    "\n",
    "WCSS is defined as the sum of the squared distance between each member of the cluster and its centroid.\n",
    "\n",
    "![image.png](https://miro.medium.com/max/1400/1*vLTnh9xdgHvyC8WDNwcQQw.png)\n",
    "\n",
    "For example, the computed WCSS for figure 1 would be greater than the WCSS calculated for figure 2.\n",
    "\n",
    "![img.png](https://miro.medium.com/max/1400/1*0naSz4RFw_m5VqiRXo2SRw.png)\n",
    "Figure 1\n",
    "\n",
    "![img2.png](https://miro.medium.com/max/2000/1*vNsFrDUvGn9yTjlnXLgW8A.png)\n",
    "Figure 2\n",
    "\n",
    "Image credits: miro.medium.com\n",
    "\n",
    "\n",
    "#### Now lets see how we can implement a k-means algorithm using python.\n",
    "\n",
    "We start by importing libraries.\n",
    "\n",
    "Q1. Write code to import:\n",
    "- pandas\n",
    "- numpy\n",
    "- pyplot from matplotlib\n",
    "- KMeans from sklearn.cluster\n",
    "- make_blobs from sklearn.datasets.samples_generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will perform the first trial on a dataset generated using the make_blobs function.\n",
    "This function is available in the sklearn.datasets module. The 'centres' parameter specifies the number of clusters.\n",
    "Changing the 'random_state' will change the data points you obtain. 'n_samples' controls number of data points generated and 'cluster_std' controls the 'tightness' of the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q.2. Display the created datset using scatter function in pyplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#A2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though we already know the optimal number of clusters, we could still benefit from determining it using the elbow method. \n",
    "\n",
    "To get the values used in the graph, we train multiple models using a different number of clusters and storing the value of the intertia_ property (WCSS) every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wcss = []\n",
    "for i in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=0)\n",
    "    kmeans.fit(X)\n",
    "    wcss.append(kmeans.inertia_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q.3. Write pyplot lines of code to visualise the graph of elbow method.\n",
    "- plot the wcss vs number of cluster graph from range 1 to 11\n",
    "- change the labels of x and y axis to appropriately. (X axis should be Number of clusters and Y axis should be wcss).\n",
    "- chage the title of the plot to 'Elbow Method Graph'\n",
    "- display the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#A3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we’ll categorize the data using the optimum number of clusters (4) we determined in the last step. k-means++ ensures that you get don’t fall into the random initialization trap.\n",
    "\n",
    "Read more about the random initialisation trap here:\n",
    "https://www.geeksforgeeks.org/ml-k-means-algorithm/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=4, init='k-means++', max_iter=300, n_init=10, random_state=0)\n",
    "pred_y = kmeans.fit_predict(X)\n",
    "\n",
    "plt.scatter(X[:,0], X[:,1])\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=300, c='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Great ! So far, we used the sklearn library to fit the k-means model easily.\n",
    "\n",
    "Here is an exercise that will allow you to actually visualise the working of the algorithm.\n",
    "\n",
    "Q4. Develop code to compute k-means cluster with k known apriori.\n",
    "Randomly take a 2d dataset of 20-25 points. (You can choose to take the points carefully so that they give you some good clustering). Take k=3/4. Write code that minimises inter-cluster error and finds the optimal centres. Make sure you plot graphs wherever you can to visualise the algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#A4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wonderful, now lets try using k-means on a real life dataset\n",
    "\n",
    "We will be using the Titanic dataset (available[here](https://www.kaggle.com/c/titanic)). \n",
    "\n",
    "Some information about the data:\n",
    "- The training set contains several records about the passengers of Titanic (hence the name of the dataset). \n",
    "- It has 12 features capturing information about passenger_class, port_of_Embarkation, passenger_fare etc. \n",
    "- The dataset's label is survival which denotes the survivial status of a particular passenger. \n",
    "- Your task is to cluster the records into two i.e. the ones who survived and the ones who did not.\n",
    "\n",
    "You might be thinking that since it is a labeled dataset, how could it be used for a clustering task? You just have to drop the 'survival' column from the dataset and make it unlabeled. It's the task of K-Means to cluster the records of the datasets if they survived or not. The label is provided to allow other supervised analysis algorithms to be carried out on the data.\n",
    "\n",
    "Q.5 Import dependancies (No need to re-import if done previously)\n",
    "\n",
    "- pandas as pd\n",
    "- numpy as np\n",
    "- KMeans from sklearn.cluster\n",
    "- LabelEncoder and MinMaxScaler from sklearn.preprocessing\n",
    "- seaborn\n",
    "- pyplot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#A5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q.6. Load data using pandas from the following URLs:\n",
    "[train](http://s3.amazonaws.com/assets.datacamp.com/course/Kaggle/train.csv)\n",
    "[test](http://s3.amazonaws.com/assets.datacamp.com/course/Kaggle/test.csv) \n",
    "\n",
    "Preview the dataset by printing some samples and get some initial statistics of the data such as count, mean, std etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#A6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to note that k-means algorithm cannot handle missing values, so we need to deak with them at the pre-processing state.\n",
    "\n",
    "There are a couple of ways to handle missing values:\n",
    "\n",
    "    Remove rows with missing values\n",
    "    Impute missing values\n",
    "\n",
    "Latter one is preferred because if you remove the rows with missing values it can cause insufficiency in the data which in turn results in inefficient training of the machine learning model.\n",
    "\n",
    "Now, there are several ways you can perform the imputation:\n",
    "\n",
    "    A constant value that has meaning within the domain, such as 0, distinct from all other values.\n",
    "    A value from another randomly selected record.\n",
    "    A mean, median or mode value for the column.\n",
    "    A value estimated by another machine learning model.\n",
    "\n",
    "Any imputation performed on the train set will have to be performed on test data in the future when predictions are needed from the final machine learning model. This needs to be taken into consideration when choosing how to impute the missing values.\n",
    "\n",
    "Q.7. Find which fields contain missing value and impute them with mean of the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#A7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have imputed the missing values in the dataset, it's time to see if the dataset still has any missing values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(train.isnull().sum())\n",
    "print(test.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, you can see there are still some missing values in the Cabin and Embarked columns. \n",
    "This is because these values are non-numeric.\n",
    "In order to perform the imputation the values need to be in numeric form. There are ways to convert a non-numeric value to a numeric one.\n",
    "\n",
    "You can see that the following features are non-numeric:\n",
    "\n",
    "    Name\n",
    "    Sex\n",
    "    Ticket\n",
    "    Cabin\n",
    "    Embarked\n",
    "\n",
    "Before converting them into numeric ones, you might want to do some feature engineering, i.e. features like Name, Ticket, Cabin and Embarked do not have any impact on the survival status of the passengers. Often, it is better to train your model with only significant features than to train it with all the features, including unnecessary ones. It not only helps in efficient modelling, but also the training of the model can happen in much lesser time. \n",
    "\n",
    "Features: Name, Ticket, Cabin and Embarked can be dropped and they will not have significant impact on the training of the K-Means model.\n",
    "\n",
    "Q.8 Drop the unnecessary fields mentioned above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#A8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the dropping part is done let's convert the 'Sex' feature to a numerical one (only 'Sex' is remaining now which is a non-numeric feature). You will do this using a technique called Label Encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labelEncoder = LabelEncoder()\n",
    "labelEncoder.fit(train['Sex'])\n",
    "labelEncoder.fit(test['Sex'])\n",
    "train['Sex'] = labelEncoder.transform(train['Sex'])\n",
    "test['Sex'] = labelEncoder.transform(test['Sex'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q.9. Investigate the datatype of all remaining features to ensure that everythin is in numeric format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#A9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# drop the Survival column from the data with the drop() function.\n",
    "\n",
    "X = np.array(train.drop(['Survived'], 1).astype(float))\n",
    "y = np.array(train['Survived'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q.10. Cluster the data using KMeans from sklearn with k=2 (Because we want two clusters: survived and not survived)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#A10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see all the other parameters of the model other than n_clusters. \n",
    "\n",
    "Let's see how well the model is doing by looking at the percentage of passenger records that were clustered correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct = 0\n",
    "for i in range(len(X)):\n",
    "    predict_me = np.array(X[i].astype(float))\n",
    "    predict_me = predict_me.reshape(-1, len(predict_me))\n",
    "    prediction = kmeans.predict(predict_me)\n",
    "    if prediction[0] == y[i]:\n",
    "        correct += 1\n",
    "\n",
    "print(correct/len(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your model was able to cluster correctly with a 50% (accuracy of your model). But in order to enhance the performance of the model you could tweak some parameters of the model itself. I will list some of these parameters which the scikit-learn implementation of K-Means provides:\n",
    "\n",
    "    algorithm\n",
    "    max_iter\n",
    "    n_jobs \n",
    " \n",
    "Let's tweak the values of these parameters and see if there is a change in the result.\n",
    "\n",
    "In the scikit-learn documentation, you will find a solid information about these parameters which you should dig further.\n",
    "\n",
    "One of the reasons of low accuracy is you have not scaled the values of the different features that you are feeding to the model. The features in the dataset contain different ranges of values. So, what happens is a small change in a feature does not affect the other feature. So, it is also important to scale the values of the features to a same range.\n",
    "\n",
    "Let's do that now and for this experiment you are going to take 0 - 1 as the uniform value range across all the features.\n",
    "\n",
    "Q.11. Use the MinMaxScaler() and transform() fucntion to normalise all values to the range of 0-1\n",
    "\n",
    "Q.12. Fit the kmeans on the new tranformed features and find the accuracy as we did previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#A11\n",
    "#A12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Well Done !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
