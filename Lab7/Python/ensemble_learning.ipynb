{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Learning\n",
    "\n",
    "Ensemble learning techniques attempt to make the performance of the predictive models better by improving their accuracy. Ensemble Learning is a process using which multiple machine learning models (such as classifiers) are strategically constructed to solve a particular problem.\n",
    "The goal of ensemble methods is to combine the predictions of several base estimators built with a given learning algorithm in order to improve generalizability / robustness over a single estimator.\n",
    "\n",
    "### Methods of Ensemble Learning\n",
    "The three most popular methods for combining the predictions from different models are:\n",
    "\n",
    "1. Bagging: Building multiple models (typically of the same type) from different subsamples of the training dataset.\n",
    "2. Boosting: Building multiple models (typically of the same type) each of which learns to fix the prediction errors of a prior model in the chain.\n",
    "3. Voting: Building multiple models (typically of differing types) and simple statistics (like calculating the mean) are used to combine predictions.\n",
    "\n",
    "We will be looking at methods 1 and 3 in this Labsheet.\n",
    "\n",
    "##### Important Notes: \n",
    "1. You will be required your previously acquired knowledge of functions of Libraries to code in this labsheet. \n",
    "2. While it is not expected that you might remember all functionalities, Remember: \"With great practice comes great memory\" and \"Google is everyone's best friend (as long as it is not the labtest).\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1: Bagging Algorithms\n",
    "\n",
    "Bagging (aka Bootstrap Aggregation) involves taking multiple samples from your training dataset (with replacement) and training a model for each sample.\n",
    "\n",
    "The final output prediction is averaged across the predictions of all of the sub-models.\n",
    "\n",
    "The two bagging models covered in this section are as follows:\n",
    "\n",
    "1. Bagged Decision Trees\n",
    "2. Random Forest\n",
    "\n",
    "#### 1.1 Bagged Decision Trees\n",
    "Bagging performs best with algorithms that have high variance. A popular example are decision trees, often constructed without pruning.\n",
    "The following example uses Bagging Classifier with 100 Decision Trees\n",
    "##### Notes:\n",
    "1. High variance means that your estimator (or learning algorithm) varies a lot depending on the data that you give it. If algorithm is able to fit your data extremely well every single time and even a single data point perturbation changes the algorithm a lot then the algorithm is has high variance. This type of high variance is called __overfitting__. Thus usually overfitting is related to high variance. This is bad because it means your algorithm is probably not robust to noise.\n",
    "2. A decision tree has high variance because, if you imagine a very large tree, it can basically adjust its predictions to every single input. It is sensitive to where it splits and how it splits. Therefore, even small changes in input variable values might result in very different tree structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1: Import required libraries\n",
    "You'll need the following libraries: \n",
    "pandas,  model_selection (available in sklearn), BaggingClassifier (available in sklearn.ensemble) and DecisionTreeClassifier (available in sklearn.tree). \n",
    "\n",
    "Hint: import statements can be written as-\n",
    "- import x, \n",
    "- import x as y, \n",
    "- from a import b. \n",
    "\n",
    "Figure out how the necessary imports will be done in this case.\n",
    "\n",
    "Write code to import these dependencies below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#(1)code for step 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2: Reading the Dataset\n",
    "Now, you need to read the .csv file of the Letter Image Recognition Data.\n",
    "Remember: You can (and should for all purposes of this lab) use pandas to tinker with data whenever required.\n",
    "For reading a csv file, typically the following steps are followed:\n",
    "1. specify the url/file path to the dataset\n",
    "2. make an array of strings that specify the column-names of the dataset (Note this step is option but recommended for the readability of your code).\n",
    "3. use the variables from 1 and 2 to call an appropriate function in pandas to read the csv file. [Hint: this function has a very obvious name, you used it in previous labsheets and it returns a dataframe object].\n",
    "\n",
    "You could now verify that your dataset has been read correctly by printing the first few rows(remember - head?)\n",
    "\n",
    "Write code to read the dataset below:\n",
    "\n",
    "##### Note: read_csv() fuction reads both .csv file and .txt file. Other arguments of the fuction remain same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#(2)code for steps 2.1,2.2 and 2.3 goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3: Getting ready before actual classification\n",
    "\n",
    "Once data has been read, the next step is always pre-processing.\n",
    "Though in this dataset, we fortunately do not require any pre-processing but it is good practice to be well-versed with the metadata file of the dataset to ensure that you carry out necessary processing steps before using the data.\n",
    "\n",
    "We now store the values from the dataframe object in an array and then seperate it into input attributes and target class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "array = df.values \n",
    "#dataframe is the name of the variable that you assigned to the object returned when you used the read_csv.\n",
    "\n",
    "X = array[:,1:16] \n",
    "#starting at index 0, columns 1 to 16 form the input fields\n",
    "\n",
    "y = array[:,0]\n",
    "#the zeroth column is the target class\n",
    "\n",
    "### ANOTHER way to get X and y (What we were already doing in previous lab sheets) ###\n",
    "# X = df.iloc[:,1:]\n",
    "# y = df.iloc[:,1]\n",
    "# Former one generate numpy array and latter one, panda dataframe (if multiple colums selected) or series (if only one column)\n",
    "\n",
    "# You can use print(type(X/y)) to check data types of both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 4: Split the dataset into train and test data\n",
    "\n",
    "The __‘train_test_split’__ function of Scikit-learn model_selection class takes in 5 parameters. The first two parameters are the input and target data we split up earlier. Next, we will set ‘test_size’ to 0.3. This means that 30% of all the data will be used for testing, which leaves 70% of the data as training data for the model to learn from.\n",
    "Setting ‘stratify’ to y makes our training split represent the proportion of each value in the y variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.3, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ---------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check accuracy of model on single decision tree\n",
    "\n",
    "Before creating bagging classifier, we will check the accuracy on simple decision tree model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CHECK THE ACCURACY on Testing data\n",
    "## Direct method\n",
    "print(\"Accuracy-direct method:\",dt.score(X_test,y_test))\n",
    "\n",
    "## Another similar method\n",
    "y_pred = dt.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Accuracy:\",accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ---------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 5: Creating the bagging decision tree classifier\n",
    "\n",
    "We will use 10 folds with a random seed and 100 decision trees\n",
    "\n",
    "Note: It is a good idea to look up the return types and details from documentation of sklearn for the funtions we will use to get a better understanding of what is happening\n",
    "\n",
    "__Important: Before poceeding, please read this [article](https://towardsdatascience.com/why-and-how-to-cross-validate-a-model-d6424b45261f) fully.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Aprroach 1: Train and Test Split\n",
    "\n",
    "seed = 7  #you can use any number, or even generate a random number if you fancy\n",
    "number_of_trees = 100\n",
    "\n",
    "dtree_1 = DecisionTreeClassifier()\n",
    "\n",
    "baggin_model_1 = BaggingClassifier(base_estimator=dtree_1, n_estimators=number_of_trees, random_state=seed)\n",
    "baggin_model_1.fit(X_train,y_train)\n",
    "\n",
    "# Check accuracy on testing data\n",
    "print(\"Accuracy: \",baggin_model_1.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Aprroach 2: K-Fold\n",
    "\n",
    "kfold = model_selection.KFold(n_splits = 10, random_state = seed)\n",
    "#here n_splits=10 because we are doing a 10-fold verification. \n",
    "\n",
    "dtree_2 = DecisionTreeClassifier()\n",
    "\n",
    "bagging_model_2 = BaggingClassifier(base_estimator=dtree_2, n_estimators=number_of_trees, random_state=seed)\n",
    "\n",
    "results = model_selection.cross_val_score(bagging_model_2, X, y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Note:\n",
    "1. __cross_val_score() fuction automatically do the split and print() statement prints the mean of accuracies on testing data.__\n",
    "2. __Remove mean() function to see all 10 folds accuracy individually.__\n",
    "\n",
    "You could now try changing some values in the above code and see how your model changes and try to figure out what gives best results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Random Forest\n",
    "Random forest is an extension of bagged decision trees.\n",
    "\n",
    "Samples of the training dataset are taken with replacement, but the trees are constructed in a way that reduces the correlation between individual classifiers. Specifically, rather than greedily choosing the best split point in the construction of the tree, only a random subset of features are considered for each split.\n",
    "\n",
    "##### Step 1: Import required libraries\n",
    "You'll need the following libraries: \n",
    "pandas,  model_selection (available in sklearn) RandomForestClassifier (available in sklearn.ensemble)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#(3)code for step 1 here\n",
    "#note: you need not re-import a library that you may have already imported\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step  2, 3 and 4: \n",
    "Remains same as before, so we wont go through the trouble of writing those lines of code again.\n",
    "we will re-use the variables X, y and kfold and Num_of_trees.\n",
    "\n",
    "##### Step 5: Creating the Random Forest Classifier\n",
    "We use Test-Train split/10-fold verification and 100 trees with max features in each tree as 3 (max_features is the size of the random subsets of features to consider when splitting a node.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Aprroach 1: Train and Test Split\n",
    "\n",
    "max_features = 5\n",
    "\n",
    "rf_model_1 = RandomForestClassifier(n_estimators=number_of_trees, max_features=max_features)\n",
    "\n",
    "#(4)fit the model and check accuracy(similar to what was done in bagging)\n",
    "## code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Aprroach 2: K-Fold\n",
    "\n",
    "rf_model_2 = RandomForestClassifier(n_estimators=number_of_trees, max_features=max_features)\n",
    "\n",
    "#(5)check the score\n",
    "## code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2: Voting Ensemble\n",
    "\n",
    "Voting is one of the simplest ways of combining the predictions from multiple machine learning algorithms.\n",
    "\n",
    "It works by first creating two or more standalone models from your training dataset. A Voting Classifier can then be used to wrap your models and average the predictions of the sub-models when asked to make predictions for new data.\n",
    "\n",
    "The predictions of the sub-models can be weighted, but specifying the weights for classifiers manually or even heuristically is difficult. More advanced methods can learn how to best weight the predictions from submodels, but this is called stacking (stacked generalization) and is currently not provided in scikit-learn.\n",
    "\n",
    "You can create a voting ensemble model for classification using the VotingClassifier class.\n",
    "\n",
    "We will create a simple ensemble voting model using two base models: Logistic Regression and Decision Tree Classifier.\n",
    "More complex Voting Models can be created using with more number and complexity of base classifiers.\n",
    "\n",
    "##### Step 1: Import required libraries\n",
    "You'll need the following libraries: \n",
    "pandas,  model_selection (available in sklearn), LogisticRegression (available in sklearn.linear_model), DecisionTreeClassifier (available in sklearn.tree) and VotingClassifier (available in sklearn.ensemble)\n",
    "\n",
    "Note: do not import libraries already imported before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#(6)code for step 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step  2 and 3: \n",
    "Remains same as before, so we wont go through the trouble of writing those lines of code again.\n",
    "we will re-use the variables X, Y, seed and kfold.\n",
    "\n",
    "##### Step 4: Creating the Voting Classifier\n",
    "\n",
    "We will now create two seperate base classifiers i.e. the Decision Tree and Logistic Regression models and combine them using Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create the sub models\n",
    "estimators = []\n",
    "base1 = LogisticRegression()\n",
    "estimators.append(('logistic', base1))\n",
    "base2 = DecisionTreeClassifier()\n",
    "estimators.append(('cart', base2))\n",
    "\n",
    "# create the ensemble model\n",
    "ensemble = VotingClassifier(estimators)\n",
    "voting_results = model_selection.cross_val_score(ensemble, X, y, cv=kfold)\n",
    "print(voting_results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### That's All Folks!\n",
    "\n",
    "Similar simple modeling example with greater clarity. Please go through it - [reference](https://www.pluralsight.com/guides/ensemble-modeling-scikit-learn).\n",
    "\n",
    "To know more about Bagging: [GFG](https://www.geeksforgeeks.org/ml-bagging-classifier/)\n",
    "\n",
    "More about all Ensemble learning :\n",
    "1. [Scikit-learn documentaion](https://scikit-learn.org/stable/modules/ensemble.html)\n",
    "2. [Comprehensive Guide](https://www.analyticsvidhya.com/blog/2018/06/comprehensive-guide-for-ensemble-models/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__This Labsheet has been compiled using the following [reference](https://machinelearningmastery.com/ensemble-machine-learning-algorithms-python-scikit-learn/).__"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "-------------------------------------ANSWERS---------------------------------------\n",
    "\n",
    "(1)\n",
    "import pandas as pd\n",
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "(2)\n",
    "df = pd.read_csv('letter-recognition-data.txt',header = None, names=[\"lettr\",\"x-box\",\"y-box\",\"width\",\"high\",\"onpix\",\"x-bar\",\"y-bar\",\"x2bar\",\"y2bar\",\"xybar\",\"x2ybr\",\"xy2br\",\"x-ege\",\"xegvy\",\"y-ege\",\"yegvx\"])\n",
    "df.head()\n",
    "\n",
    "(3)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "(4)\n",
    "rf_model_1.fit(X_train,y_train)\n",
    "# Check accuracy on testing data\n",
    "print(rf_model_1.score(X_test,y_test))\n",
    "\n",
    "(5)\n",
    "results = model_selection.cross_val_score(rf_model_2, X, y, cv=kfold)\n",
    "print(results.mean())\n",
    "\n",
    "(6)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import VotingClassifier"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
