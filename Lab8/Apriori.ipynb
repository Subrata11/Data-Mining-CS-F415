{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apriori in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apriori algorithm works on the principle of Association Rule Mining. Association rule mining is a technique to identify underlying relations between different items. This relationship can be a similarity between items on how frequently they are bought or how similar users bought it.\n",
    "\n",
    "In this labsheet, we will be looking on how the Apriori algorithm works with a python example.\n",
    "\n",
    "## Apriori Algorithm\n",
    "\n",
    "There are three major components of the Apriori algorithm:\n",
    "1. Support\n",
    "2. Confidence\n",
    "3. Lift\n",
    "\n",
    "We will explain this concept with the help of an example.\n",
    "\n",
    "Suppose we have a record of 1000 customers transactions and we want to find out support, confidence and lift for milk and diapers. out of 1000 transactions, 120 contains a milk and 150 contains a diaper. out of this 150 transaction where a diaper is purchased 30 contains transaction contains milk as well. we will use this data to calculate support, confidence and lift.\n",
    "\n",
    "### Support\n",
    "\n",
    "Support refers to the popularity of item and can be calculated by finding the number of transactions containing a particular item divided by the total number of transactions.\n",
    "\n",
    "**Support(diaper) = (Transactions containing (diaper))/(Total Transactions)**\n",
    "\n",
    "### Confidence\n",
    "\n",
    "Confidence refers to the likelihood that an item B is also bought if item A is bought. It can be calculated by finding the number of transactions where A and B are bought together, divided by the total number of transactions where A is bought. Mathematically, it can be represented as:\n",
    "\n",
    "**Confidence(A → B) = (Transactions containing both (A and B))/(Transactions containing A)**\n",
    "\n",
    "### Lift\n",
    "\n",
    "Lift refers to the increase in the ratio of the sale of B when A is sold.\n",
    "\n",
    "**Lift(A→B) = (Confidence (A→B))/(Support (B))**\n",
    "\n",
    "\n",
    "Association rule by Lift\n",
    "\n",
    "lift = 1 → There is no association between A and B.\n",
    "\n",
    "lift < 1 → A and B are unlikely to be bought together.\n",
    "\n",
    "lift > 1 → greater the lift greater is the likelihood of buying both products together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run this file make sure you have installed mlextend library in anaconda environment.\n",
    "To install it, run command in your anaconda prompt:\n",
    "\n",
    "       conda install -c conda-forge mlxtend "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries and Preparing Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing Libraries\n",
    "import pandas as pd\n",
    "from mlxtend.preprocessing import OnehotTransactions\n",
    "from mlxtend.frequent_patterns import apriori,association_rules\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the dataset\n",
    "dataset = [['Milk', 'Onion', 'Nutmeg', 'Kidney Beans', 'Eggs', 'Yogurt'],\n",
    "           ['Dill', 'Onion', 'Nutmeg', 'Kidney Beans', 'Eggs', 'Yogurt'],\n",
    "           ['Milk', 'Apple', 'Kidney Beans', 'Eggs'],\n",
    "           ['Milk', 'Unicorn', 'Corn', 'Kidney Beans', 'Yogurt'],\n",
    "           ['Corn', 'Onion', 'Onion', 'Kidney Beans', 'Ice cream', 'Eggs']]\n",
    "            \n",
    "            \n",
    "oht = OnehotTransactions()\n",
    "oht_ary = oht.fit(dataset).transform(dataset)\n",
    "df = pd.DataFrame(oht_ary, columns=oht.columns_)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Frequent Itemsets and Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating frequent Itemsets\n",
    "frequent_itemsets = apriori(df, min_support=0.6, use_colnames=True)\n",
    "frequent_itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating Rules\n",
    "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=.8)\n",
    "rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Plot graph for rules\n",
    "def draw_graph(rules):\n",
    "    rules_to_show = len(rules)\n",
    "    G1 = nx.DiGraph()\n",
    "\n",
    "    color_map=[]\n",
    "    N = 50\n",
    "    colors = np.random.rand(N)    \n",
    "    strs=['R0', 'R1', 'R2', 'R3', 'R4', 'R5', 'R6', 'R7', 'R8', 'R9', 'R10', 'R11']   \n",
    "\n",
    "\n",
    "    for i in range (rules_to_show):      \n",
    "        G1.add_nodes_from([\"R\"+str(i)])\n",
    "\n",
    "\n",
    "        for a in rules.iloc[i]['antecedents']:\n",
    "\n",
    "            G1.add_nodes_from([a])\n",
    "\n",
    "            G1.add_edge(a, \"R\"+str(i), color=colors[i] , weight = 2)\n",
    "\n",
    "        for c in rules.iloc[i]['consequents']:\n",
    "\n",
    "                G1.add_nodes_from([c])\n",
    "\n",
    "                G1.add_edge(\"R\"+str(i), c, color=colors[i],  weight=2)\n",
    "\n",
    "    for node in G1:\n",
    "        found_a_string = False\n",
    "        for item in strs: \n",
    "            if node==item:\n",
    "                found_a_string = True\n",
    "        if found_a_string:\n",
    "            color_map.append('yellow')\n",
    "        else:\n",
    "            color_map.append('green')       \n",
    "\n",
    "\n",
    "\n",
    "    edges = G1.edges()\n",
    "    colors = [G1[u][v]['color'] for u,v in edges]\n",
    "    weights = [G1[u][v]['weight'] for u,v in edges]\n",
    "\n",
    "    pos = nx.spring_layout(G1, k=16, scale=1)\n",
    "    nx.draw(G1, pos, edges=edges, node_color = color_map, edge_color=colors, width=weights, font_size=16, with_labels=False)            \n",
    "\n",
    "    for p in pos:  # raise text positions\n",
    "        pos[p][1] += 0.07\n",
    "    nx.draw_networkx_labels(G1, pos)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_graph (rules)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
